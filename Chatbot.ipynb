{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdc8caf-a3e8-4d5a-8c4b-4133ec070ec4",
   "metadata": {},
   "source": [
    "# A simple chatbot using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40fa3b-2839-4052-9983-c526678dc4b8",
   "metadata": {},
   "source": [
    "Simple Chatbot scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0ca6d-d928-4932-83b2-564974ff2eb5",
   "metadata": {},
   "source": [
    "In 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published, which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109824e5-4371-4eb4-bd92-00559d37ab77",
   "metadata": {},
   "source": [
    " ELIZA, published in 1966,by Weizenbaum, seemed to be able to fool users into believing that they were conversing with a real human. It imitated the language of a psychotherapist from only 200 lines of code. [Link for ELIZA](https://www.google.com/url?q=http%3A%2F%2Fpsych.fullerton.edu%2Fmbirnbaum%2Fpsych101%2FEliza.htm%3Futm_source%3Dubisend.com%26utm_medium%3Dblog-link%26utm_campaign%3Dubisend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a9f0f-5343-4c0c-a92a-f8066b7b3586",
   "metadata": {},
   "source": [
    "The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65acd97e-58d1-4458-8a33-6487395adf22",
   "metadata": {},
   "source": [
    "Other : PARRY (1972), A.L.I.C.E, D.U.D.E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d2ae99-0d65-4b64-8cca-4bd8a85431b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io                  #reading and writing files or handling streams of data.\n",
    "import random              #generating random numbers and selecting random elements from a sequence.\n",
    "import string              #provides functions and constants related to string operations\n",
    "import warnings            #to control warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#to convert a collection of text documents into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#to calculate the similarity between two vectors using cosine similarity.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e35e5-8440-4feb-b488-254a1cfd905d",
   "metadata": {},
   "source": [
    "TF-IDF helps to give importance to words that are frequent in a document but rare in the entire dataset.\n",
    "It consists of two components:\n",
    "\n",
    "**Term Frequency (TF):** Measures how often a word appears in a document. (document = sentence) (corpus = collection of documents)\n",
    "\n",
    "TF = Number of times a word appears in the document/ Total words in the document\n",
    "\n",
    "​**Inverse Document Frequency (IDF):** Measures how unique a word is across all documents.\n",
    "\n",
    "IDF = log(Total number of documents/ Number of documents containing the word)\n",
    "\n",
    "Rare words get higher IDF scores.\n",
    "Common words (like \"is\", \"the\", \"and\") get lower IDF scores.\n",
    "\n",
    "TF-IDF also have disadvantage of semantic words. (better and good are considered different). To overcome this here, lemmatization helps in creating single words for semantic words (through wordnet dictionary already available)\n",
    "\n",
    "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bb22c5-baf0-4f5c-8e10-35268c2c6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ab3750-e222-48f2-a6c3-dcde436801b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # downloading packages - punktokenizer , wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bf5098-4957-4446-8d7e-09129bf7e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"chatbot.txt\", \"r\", errors = 'ignore')\n",
    "data = fh.read()\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaf03a-762d-4a5b-8232-1810d8ee9f12",
   "metadata": {},
   "source": [
    "The main issue with text data is that it is all in text format (strings). However, the Machine learning algorithms need some sort of numerical feature vector in order to perform the task. Converting the entire text into uppercase or lowercase, so that the algorithm does not treat the same words in different cases as different.\n",
    "\n",
    "Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "\n",
    "Stemming: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”. It remove suffix.\n",
    "\n",
    "Lemmatization: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same. (based on verb, noun, adjective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bc1d3-d8af-4562-b2f2-bbe27e229bc9",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b151e2-aca7-4b2f-871a-a265cfae0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74417625-e548-477d-9e36-5de9a37517f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f638c42-3359-41fb-8e4b-6d88b5213fd1",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8cfb5f-2984-4163-8ea6-282be091032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function is defined called LemTokens which will take as input the tokens and return normalized tokens.- lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNetLemmatizer() is used to reduce words to their base or dictionary form (lemma). Lemmatization\n",
    "\n",
    "def lemtokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "    #It takes a list of words (tokens) and applies lemmatization to each word.\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "#To creates a dictionary that maps punctuation ASCII values to None\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return lemtokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "#To convert to lowercase using text.lower(), To Remove punctuation using text.translate(remove_punct_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ade06b-5007-44e6-b387-afc1de969cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "lemmer.lemmatize('running', pos='v')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf09c00-a49f-48fe-a487-06872229dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = ('hello','hi','greetings', 'hey','holla')\n",
    "GREETING_RESPONSES = [\"hi\",\"hello\",\"*nods*\",\"hey\",\" I am glad! You are talking.\"]\n",
    "def greetings(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae3711c-28ed-4542-bad7-f95cd5f96fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "greetings(\"Hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5392cf-7f50-4221-8dd5-6ec93998f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    bot_response = ''\n",
    "    # to store reply\n",
    "    sentence_tokens.append(user_response)\n",
    "    # The user’s input (user_response) is added to sent_tokens, which contains all known sentences.\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    #TfidfVectorizer converts text into numerical vectors based on word importance.\n",
    "    #tokenizer=LemNormalize: Uses the previously defined LemNormalize() function to preprocess text (lemmatization, tokenization, and punctuation removal).\n",
    "    #stop_words='english': Removes common words like \"is,\" \"the,\" \"and\" to focus on meaningful words.\n",
    "    #fit_transform(sent_tokens): Computes TF-IDF weights for all sentences.\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    #tfidf[-1] represents the TF-IDF vector of the user's input.\n",
    "    #cosine_similarity(tfidf[-1], tfidf) computes the similarity between the user’s input and all sentences in sent_tokens.\n",
    "    #The result vals is an array where higher values indicate more similar sentences.\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    #argsort() sorts similarity scores in ascending order. [-2] selects the second last value (most similar sentence).[-1] would be the user’s own input (which will always be the most similar to itself).\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    #Flattens the similarity array to a 1D list.Sorts the values in ascending order.Selects the second highest similarity score (req_tfidf).\n",
    "    if req_tfidf == 0:\n",
    "     bot_response = bot_response + \"I am sorry! I don't understand you\"\n",
    "     return bot_response\n",
    "    else:\n",
    "     bot_response = bot_response + sentence_tokens[idx]\n",
    "     return bot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04562cb-da16-4acd-b3ed-676f629548fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: My name is bot. I will answer your queries about Chatbots. If you want to exit, type Bye!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT:  I am glad! You are talking.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Benefits of using chatbots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: benefits of using chatbots\n",
      "improved customer service: provide 24/7 support, answer frequently asked questions, and assist customers quickly.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Challenges of using chatbots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: challenges of using chatbots\n",
      "limited understanding: may struggle with complex or nuanced queries.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag = True #to control loop - chatbot keeps running bcz od true\n",
    "print(\"BOT: My name is bot. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag = False\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        else:\n",
    "             if(greetings(user_response)!=None):\n",
    "                print(\"BOT: \"+greetings(user_response))\n",
    "             else:\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400bfd9-3915-43be-b1ca-c7a2401eaa02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
